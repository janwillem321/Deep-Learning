{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6233846,"sourceType":"datasetVersion","datasetId":3581083}],"dockerImageVersionId":30674,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport torch.nn as nn\nfrom torchinfo import summary\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nfrom torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision.utils import save_image\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\n\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-28T11:12:55.864024Z","iopub.execute_input":"2024-03-28T11:12:55.864462Z","iopub.status.idle":"2024-03-28T11:13:15.375438Z","shell.execute_reply.started":"2024-03-28T11:12:55.864435Z","shell.execute_reply":"2024-03-28T11:13:15.374656Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-03-28 11:13:05.378069: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-28 11:13:05.378171: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-28 11:13:05.555903: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"class PairedImageDataset(Dataset):\n    def __init__(self, rootA, rootB, transform=None):\n        \"\"\"\n        Args:\n            rootA (string): Directory with all the images in trainA.\n            rootB (string): Directory with all the images in trainB.\n            transform (callable, optional): Optional transform to be applied on a sample.\n        \"\"\"\n        self.rootA = rootA\n        self.rootB = rootB\n        self.transform = transform\n\n        # Assuming filenames in both folders are the same and in order\n        self.filenames = sorted(os.listdir(rootA))\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self, idx):\n        img_nameA = os.path.join(self.rootA, self.filenames[idx])\n        imageA = Image.open(img_nameA).convert('RGB')\n        \n        img_nameB = os.path.join(self.rootB, self.filenames[idx])\n        imageB = Image.open(img_nameB).convert('RGB')\n\n        if self.transform:\n            imageA = self.transform(imageA)\n            imageB = self.transform(imageB)\n\n        return imageA, imageB","metadata":{"execution":{"iopub.status.busy":"2024-03-28T11:13:15.376883Z","iopub.execute_input":"2024-03-28T11:13:15.377378Z","iopub.status.idle":"2024-03-28T11:13:15.387804Z","shell.execute_reply.started":"2024-03-28T11:13:15.377353Z","shell.execute_reply":"2024-03-28T11:13:15.386924Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, latent_dims, s_img, hdim, kernel_size = (4,4), stride =2, padding = 0):\n        super(Encoder, self).__init__()\n\n        self.layers = nn.ModuleList()\n\n        # layer 1\n        self.layers.append(\n            nn.Conv2d(\n                in_channels=hdim[0],\n                out_channels=hdim[1],\n                kernel_size=kernel_size,\n                stride=stride,\n                padding=padding,\n            )\n        )\n\n        self.layers.append(nn.LeakyReLU(0.2))\n\n        # the rest of the layers\n        for in_out in range(1, (len(hdim) - 1)):\n            self.layers.append(\n                nn.Conv2d(\n                    in_channels=hdim[in_out],\n                    out_channels=hdim[(in_out + 1)],\n                    kernel_size=kernel_size,\n                    stride=stride,\n                    padding=padding,\n                )\n            )\n\n            self.layers.append(nn.BatchNorm2d(hdim[in_out + 1]))\n\n            self.layers.append(nn.LeakyReLU(0.2))\n\n\n    def forward(self, x):\n\n        SkipConnections = []\n\n        for layer in self.layers:\n            # print(layer)\n            x = layer.forward(x)\n            SkipConnections.append(x)\n\n        return x, SkipConnections","metadata":{"execution":{"iopub.status.busy":"2024-03-28T11:13:15.388825Z","iopub.execute_input":"2024-03-28T11:13:15.389086Z","iopub.status.idle":"2024-03-28T11:13:15.415243Z","shell.execute_reply.started":"2024-03-28T11:13:15.389063Z","shell.execute_reply":"2024-03-28T11:13:15.414350Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#decoder\nclass Decoder(nn.Module):\n    def __init__(self, latent_dims, s_img, hdim_in, hdim_out, kernel_size = (4,4), stride =2, padding= 0):\n        super(Decoder, self).__init__()\n\n        self.layers = nn.ModuleList()\n\n        # layers 1 to 3 with dropout\n        for in_out in range(0, 3):\n            self.layers.append(\n                nn.ConvTranspose2d(\n                    in_channels=hdim_in[in_out],\n                    out_channels=hdim_out[(in_out + 1)],\n                    kernel_size=kernel_size,\n                    stride=stride,\n                    padding=padding,\n                )\n            )\n\n            self.layers.append(nn.BatchNorm2d(hdim_out[in_out + 1]))\n            self.layers.append(nn.Dropout2d(0.5))\n            self.layers.append(nn.ReLU())\n\n        # the rest of the layers \n        for in_out in range(3, (len(hdim_in) - 1)):\n            self.layers.append(\n                nn.ConvTranspose2d(\n                    in_channels=hdim_in[in_out],\n                    out_channels=hdim_out[(in_out + 1)],\n                    kernel_size=kernel_size,\n                    stride=stride,\n                    padding=padding,\n                )\n            )\n\n            self.layers.append(nn.BatchNorm2d(hdim_out[in_out + 1]))\n            self.layers.append(nn.Identity())\n#             self.layers.append(nn.Dropout2d(0.5))\n            self.layers.append(nn.ReLU())\n        \n        \n            \n    def forward(self, z, SkipConnections):\n        \n        EncoderIndex = 3/4\n        SkipConnections.reverse()\n\n        for i, layer in enumerate(self.layers):\n            if i % 4 == 0 and i != 0:\n                j = int(EncoderIndex * i)\n                z = layer.forward(torch.add(z, SkipConnections[j]))\n#                 z = layer.forward(torch.cat((z,SkipConnections[j]), 1)) \n                    \n            else:\n                z = layer.forward(z)\n                \n        z = torch.tanh(z)\n#         z = z * 255\n        return z","metadata":{"execution":{"iopub.status.busy":"2024-03-28T11:13:15.417232Z","iopub.execute_input":"2024-03-28T11:13:15.417500Z","iopub.status.idle":"2024-03-28T11:13:15.431382Z","shell.execute_reply.started":"2024-03-28T11:13:15.417477Z","shell.execute_reply":"2024-03-28T11:13:15.430518Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Generator\nclass Generator(nn.Module):\n    def __init__(self,\n                latent_dims,\n                s_img,\n                hdim_e,\n                hdim_d_input,\n                hdim_d_output,\n                kernel_size, \n                padding):\n        super(Generator, self).__init__()\n\n        self.encoder = Encoder(\n            latent_dims=latent_dims,\n            s_img=s_img,hdim=hdim_e,\n            kernel_size= kernel_size,\n            padding= padding)\n        self.decoder = Decoder(\n            latent_dims=latent_dims,\n            s_img=s_img, \n            hdim_in=hdim_d_input, \n            hdim_out=hdim_d_output,\n            kernel_size=kernel_size, \n            padding=padding)\n\n    def forward(self, x):\n\n        z, skipConnections = self.encoder(x)\n        # print(f\"the shape of encoder is {z.shape}\")\n        y = self.decoder(z, skipConnections)\n\n        return y","metadata":{"execution":{"iopub.status.busy":"2024-03-28T11:13:15.432475Z","iopub.execute_input":"2024-03-28T11:13:15.432817Z","iopub.status.idle":"2024-03-28T11:13:15.444299Z","shell.execute_reply.started":"2024-03-28T11:13:15.432785Z","shell.execute_reply":"2024-03-28T11:13:15.443563Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    \n    def __init__(self, latent_dims, s_img, hdim, kernel_size=(4, 4), stride=2):\n        \n        super(Discriminator, self).__init__()\n\n        ########################################################################\n        #    Create the necessary layers                                 #\n        ########################################################################\n\n        self.layers = nn.ModuleList()\n        \n        # Input layer dim -- down1\n        self.layers.append(nn.Conv2d(in_channels=6, out_channels=64, kernel_size=kernel_size, stride=2, padding=1))\n\n        # Hidden to hidden convolution -- down2 and down 3\n        for i in range(0, 2):\n            self.layers.append(nn.Conv2d(in_channels=hdim[i],\n                                             out_channels=hdim[i + 1],\n                                             kernel_size=kernel_size, stride = stride, padding=1))\n\n        # Pad with zeroes\n        self.layers.append(nn.ZeroPad2d(padding=(1,1,1,1)))\n\n        # Conv2D\n        self.layers.append(nn.Conv2d(in_channels=hdim[3],\n                                             out_channels=hdim[4],\n                                             kernel_size=kernel_size, stride = 1))\n\n        # Batchnorm\n        self.layers.append(nn.BatchNorm2d(hdim[4]))\n\n        # Zeropad2\n        self.layers.append(nn.ZeroPad2d(padding=(1,1,1,1)))\n\n        #Conv2D 2\n        self.layers.append(nn.Conv2d(in_channels=hdim[5],\n                                             out_channels=hdim[6],\n                                             kernel_size=kernel_size, stride = 1))\n\n        self.Leakyrelu = nn.LeakyReLU(0.2)\n        \n    def forward(self, x):\n\n        for n_layer, layer in enumerate(self.layers):\n            ## The fourth layer first has a batchnorm and then a Leakyrelu\n            if n_layer != 4:\n                x = self.Leakyrelu(layer(x))\n            else:\n                x = layer(x)\n        return x\n        ","metadata":{"execution":{"iopub.status.busy":"2024-03-28T11:13:15.445319Z","iopub.execute_input":"2024-03-28T11:13:15.445789Z","iopub.status.idle":"2024-03-28T11:13:15.458868Z","shell.execute_reply.started":"2024-03-28T11:13:15.445757Z","shell.execute_reply":"2024-03-28T11:13:15.458165Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def train_generator(train_loader, net, optimizer, criterion, pin_memory , device = 'cpu'):\n    \"\"\"\n    Trains network for one epoch in batches.\n\n    Args:\n        train_loader: Data loader for training set.\n        net: Neural network model.\n        optimizer: Optimizer (e.g. SGD).\n        criterion: Loss function (e.g. cross-entropy loss).\n        device: whether the network runs on cpu or gpu\n    \"\"\"\n\n    avg_loss = 0\n\n    # iterate through batches\n    for i, data in enumerate(train_loader):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n        \n        # convert the inputs to run on GPU if set\n        if device != 'cpu':\n            inputs, labels = inputs.cuda(non_blocking=pin_memory), labels.cuda(non_blocking=pin_memory)\n            \n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n\n        loss = criterion(outputs, labels) * 100 #+ F.binary_cross_entropy(outputs, torch.ones(outputs.shape, device=device))\n        loss.backward()\n        optimizer.step()\n\n        # keep track of loss and accuracy\n        avg_loss += loss\n\n    return avg_loss/len(train_loader), outputs\n\ndef try_gpu():\n    \"\"\"\n    If GPU is available, return torch.device as cuda:0; else return torch.device\n    as cpu.\n    \"\"\"\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    else:\n        device = torch.device('cpu')\n    return device","metadata":{"execution":{"iopub.status.busy":"2024-03-28T11:13:15.460084Z","iopub.execute_input":"2024-03-28T11:13:15.460360Z","iopub.status.idle":"2024-03-28T11:13:15.472547Z","shell.execute_reply.started":"2024-03-28T11:13:15.460323Z","shell.execute_reply":"2024-03-28T11:13:15.471709Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def train_discriminator(train_loader, net, optimizer, criterion, pin_memory , device = 'cpu'):\n    \"\"\"\n    Trains network for one epoch in batches.\n\n    Args:\n        train_loader: Data loader for training set.\n        net: Neural network model.\n        optimizer: Optimizer (e.g. SGD).\n        criterion: Loss function (e.g. cross-entropy loss).\n        device: whether the network runs on cpu or gpu\n    \"\"\"\n    \n    # get the inputs; data is a list of [inputs, labels]\n    labels = torch.empty(output.shape)\n    x = torch.empty((1, 6, 256, 256))\n    \n    avg_loss = 0\n\n    # iterate through batches\n    for i, data in enumerate(train_loader):\n        # get the inputs; data is a list of [inputs, labels]\n        print(data.shape)\n        img_gen, input_img = data\n        \n        x[i, :, :, :] = torch.cat((img_gen, input_img), dim=0)\n        \n        labels[i, 0] = torch.zeros(30, 30)\n        \n        \n        # convert the inputs to run on GPU if set\n        if device != 'cpu':\n            inputs, labels = inputs.cuda(non_blocking=pin_memory), labels.cuda(non_blocking=pin_memory)\n            \n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n\n        loss = criterion(outputs, labels)  #+ F.binary_cross_entropy(outputs, torch.ones(outputs.shape, device=device))\n        loss.backward()\n        optimizer.step()\n\n        # keep track of loss and accuracy\n        avg_loss += loss\n\n    return avg_loss/len(train_loader)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-28T12:16:16.957375Z","iopub.execute_input":"2024-03-28T12:16:16.958427Z","iopub.status.idle":"2024-03-28T12:16:16.967913Z","shell.execute_reply.started":"2024-03-28T12:16:16.958394Z","shell.execute_reply":"2024-03-28T12:16:16.966953Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # show_images()\n    torch.cuda.empty_cache()\n    num_workers = 4\n    pin_memory = True\n    batch_size = 32\n    n_samples, in_channels, s_img, latent_dims, padding = 1, 3, 256, 512,1\n    hdim_e = [3, 64, 128, 256, 512, 512, 512, 512, 512] #choose hidden dimension encoder\n    hdim_d_output = [512, 512, 512, 512, 512, 256, 128, 64, 3]\n#     hdim_d_input = [512, 1024, 1024, 1024, 1024, 512, 256, 128, 3] #choose hidden dimension decoder\n    hdim_d_input = hdim_d_output\n    in_channels_gen = 6 # 6 for two images\n    hdim_dis = [64, 128, 256, 256, 512, 512, 1] #choose hidden dimension discriminator\n    output_shape = (n_samples, 1, 30, 30)\n    \n    kernel_size = (4,4)\n\n   # Define the transformation pipeline with added jitter\n    transform = transforms.Compose([\n        transforms.Resize(256, interpolation=transforms.InterpolationMode.NEAREST),  # Resize to 256x256 using nearest neighbor method\n        transforms.RandomCrop(256),  # Apply random cropping to introduce jitter\n        transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally\n        # Random jitter through changes in brightness, contrast, saturation, and hue\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02),\n        # Random affine transformations for scaling and rotation\n        transforms.RandomAffine(degrees=10, scale=(0.95, 1.05)),\n        transforms.ToTensor(),  # Convert images to tensor\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n    ])\n\n    # Initialize the dataset\n    paired_dataset = PairedImageDataset(rootA=r'/kaggle/input/euvp-dataset/EUVP/Paired/underwater_dark/trainA',\n                                        rootB=r'/kaggle/input/euvp-dataset/EUVP/Paired/underwater_dark/trainB',\n                                        transform=transform)\n\n    # Initialize DataLoader\n    EUVP_data = DataLoader(\n        paired_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=pin_memory)\n\n   # Fetch a single batch\n    images_a, images_b = next(iter(EUVP_data))\n#     _ , img_x = EUVP_data\n\n    #Set the number of dimensions of the latent space\n    # latent_dims = [2,3]\n    s_img = np.size(images_a[1][0], axis = 1) #get image size (height = width) from a data sample\n\n    #initialize generator model\n    model_gen = Generator(latent_dims=latent_dims,\n                        s_img=s_img,\n                        hdim_e=hdim_e, \n                        hdim_d_input=hdim_d_input,\n                        hdim_d_output=hdim_d_output, \n                        padding=padding,\n                        kernel_size=kernel_size)\n    #intit dis\n    model_dis = Discriminator(latent_dims, s_img, hdim_dis)\n\n    # Create a writer to write to Tensorboard\n    writer = SummaryWriter()\n\n    #Create instance of Autoencoder\n    device = try_gpu()\n    print(device)\n    \n    if torch.cuda.is_available():\n        model_gen= nn.DataParallel(model_gen)\n        model_gen.to(device)\n        model_dis = nn.DataParallel(model_dis)\n        model_dis.to(device)\n    # AE2 = Generator(latent_dims[0], s_img, hdim = hdim).to(device) #2-dimensional latent space\n    # AE3 = Generator(latent_dims[1], s_img, hdim = hdim).to(device) #3-dimensional latent space\n\n    # Create loss function and optimizer\n    criterion_gen = F.l1_loss \n    criterion_dis = nn.BCELoss()\n    \n    optimizer_gen = optim.Adam(model_gen.parameters(), lr=2e-4, betas=(0.5, 0.999))\n    optimizer_dis = optim.Adam(model_dis.parameters(), lr=2e-4, betas=(0.5, 0.999))\n\n    # Set the number of epochs to for training\n    epochs = 20\n    \n    plt.ion()  # Enable interactive mode\n    \n    losses_gen = []\n    losses_dis = [] \n    \n    data_set_dis = []\n    for i in range(len(outputs)):\n        data_set_dis.append((outputs[i], images_b[i].cuda(non_blocking=pin_memory)))\n        \n    \n    for epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n        # Train on data\n        train_loss_gen, outputs = train_generator(EUVP_data, model_gen, optimizer_gen, criterion_gen, pin_memory, device)\n#         print(f'outputs ={outputs.shape}')\n#         print(f'images_b ={img_x.shape}')\n        train_loss_dis = train_discriminator(data_set_dis, model_dis, optimizer_dis, criterion_gen, pin_memory, device)\n        # Write metrics to Tensorboard\n        writer.add_scalars(\"Loss\", {'Train': train_loss_gen}, epoch)\n\n        losses_gen.append(train_loss_gen.item())\n        losses_dis.append(train_loss_dis.item())\n        \n        # Visualization code\n        clear_output(wait=True)\n        plt.figure(figsize=(10, 5))\n        plt.plot(losses_gen, label='Training Loss')\n        plt.plot(losses_dis, label='Training Loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.title('Training Loss Over Time')\n        plt.legend()\n        plt.show()\n\n    plt.ioff()  # Disable interactive mode\n    writer.close()\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-28T12:16:22.653753Z","iopub.execute_input":"2024-03-28T12:16:22.654075Z","iopub.status.idle":"2024-03-28T12:17:14.167017Z","shell.execute_reply.started":"2024-03-28T12:16:22.654052Z","shell.execute_reply":"2024-03-28T12:17:14.165774Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/20 [00:48<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[30], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m         train_loss_gen, outputs \u001b[38;5;241m=\u001b[39m train_generator(EUVP_data, model_gen, optimizer_gen, criterion_gen, pin_memory, device)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m#         print(f'outputs ={outputs.shape}')\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m#         print(f'images_b ={img_x.shape}')\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m         train_loss_dis \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_discriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_set_dis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_dis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;66;03m# Write metrics to Tensorboard\u001b[39;00m\n\u001b[1;32m    105\u001b[0m         writer\u001b[38;5;241m.\u001b[39madd_scalars(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m'\u001b[39m: train_loss_gen}, epoch)\n","Cell \u001b[0;32mIn[29], line 14\u001b[0m, in \u001b[0;36mtrain_discriminator\u001b[0;34m(train_loader, net, optimizer, criterion, pin_memory, device)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mTrains network for one epoch in batches.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    device: whether the network runs on cpu or gpu\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# get the inputs; data is a list of [inputs, labels]\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\u001b[43moutput\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m))\n\u001b[1;32m     17\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n","\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"],"ename":"NameError","evalue":"name 'output' is not defined","output_type":"error"}]},{"cell_type":"code","source":"\n# make a folder\ncount = 0\nimage_counter = 10\npath = '/kaggle/working/test'\npath_loop = None\n# Initialize DataLoader\nEUVP_data = DataLoader(paired_dataset, batch_size=5, shuffle=False, num_workers=4)\n\nprint(\"start printing\")\n# Store somep images\nfor i, data in enumerate(EUVP_data):\n    inputs, labels = data\n    inputs, labels = inputs.to(device), labels.to(device)\n    y_out = model.forward(inputs)\n#     y_out = torch.tanh(y_out)\n\n    for x in range(len(inputs) - 1):\n        path_loop = f\"{path}/test{count}\"\n        if not (os.path.exists(path_loop) and os.path.isdir(path_loop)):\n            os.makedirs(path_loop)\n\n        name = (f'{path_loop}/image_in{count}.jpg')\n        save_image(inputs[x], name)\n        name = (f'{path_loop}/image_out{count}.jpg')\n        save_image(y_out[x], name)\n        name = (f'{path_loop}/image_truth{count}.jpg')\n        save_image(labels[x], name)\n        count += 1\n\n        if (count == image_counter):\n            break\n    break\n","metadata":{"execution":{"iopub.status.busy":"2024-03-28T11:13:23.551245Z","iopub.status.idle":"2024-03-28T11:13:23.551616Z","shell.execute_reply.started":"2024-03-28T11:13:23.551435Z","shell.execute_reply":"2024-03-28T11:13:23.551449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir /kaggle/working/runs","metadata":{"execution":{"iopub.status.busy":"2024-03-28T11:13:23.553006Z","iopub.status.idle":"2024-03-28T11:13:23.553313Z","shell.execute_reply.started":"2024-03-28T11:13:23.553164Z","shell.execute_reply":"2024-03-28T11:13:23.553176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" %reload_ext tensorboard","metadata":{"execution":{"iopub.status.busy":"2024-03-28T11:13:23.554386Z","iopub.status.idle":"2024-03-28T11:13:23.554712Z","shell.execute_reply.started":"2024-03-28T11:13:23.554548Z","shell.execute_reply":"2024-03-28T11:13:23.554567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!kill 292","metadata":{"execution":{"iopub.status.busy":"2024-03-28T11:13:23.556475Z","iopub.status.idle":"2024-03-28T11:13:23.556823Z","shell.execute_reply.started":"2024-03-28T11:13:23.556667Z","shell.execute_reply":"2024-03-28T11:13:23.556680Z"},"trusted":true},"execution_count":null,"outputs":[]}]}