{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6233846,"sourceType":"datasetVersion","datasetId":3581083}],"dockerImageVersionId":30674,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport torch.nn as nn\nfrom torchinfo import summary\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nfrom torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision.utils import save_image\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\n\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-04-04T04:23:12.919536Z","iopub.execute_input":"2024-04-04T04:23:12.919838Z","iopub.status.idle":"2024-04-04T04:23:32.745307Z","shell.execute_reply.started":"2024-04-04T04:23:12.919811Z","shell.execute_reply":"2024-04-04T04:23:32.744371Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-04-04 04:23:22.635609: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-04 04:23:22.635719: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-04 04:23:22.812909: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"class PairedImageDataset(Dataset):\n    def __init__(self, rootA, rootB, transform=None):\n        \"\"\"\n        Args:\n            rootA (string): Directory with all the images in trainA.\n            rootB (string): Directory with all the images in trainB.\n            transform (callable, optional): Optional transform to be applied on a sample.\n        \"\"\"\n        self.rootA = rootA\n        self.rootB = rootB\n        self.transform = transform\n\n        # Assuming filenames in both folders are the same and in order\n        self.filenames = sorted(os.listdir(rootA))\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self, idx):\n        img_nameA = os.path.join(self.rootA, self.filenames[idx])\n        imageA = Image.open(img_nameA).convert('RGB')\n        \n        img_nameB = os.path.join(self.rootB, self.filenames[idx])\n        imageB = Image.open(img_nameB).convert('RGB')\n\n        if self.transform:\n            imageA = self.transform(imageA)\n            imageB = self.transform(imageB)\n\n        return imageA, imageB","metadata":{"execution":{"iopub.status.busy":"2024-04-04T04:23:32.747073Z","iopub.execute_input":"2024-04-04T04:23:32.747619Z","iopub.status.idle":"2024-04-04T04:23:32.756842Z","shell.execute_reply.started":"2024-04-04T04:23:32.747592Z","shell.execute_reply":"2024-04-04T04:23:32.755771Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, latent_dims, s_img, hdim, kernel_size = (4,4), stride =2, padding = 1):\n        super(Encoder, self).__init__()\n\n        self.layers = nn.ModuleList()\n\n        # layer 1\n        self.layers.append(\n            nn.Conv2d(\n                in_channels=hdim[0],\n                out_channels=hdim[1],\n                kernel_size=kernel_size,\n                stride=stride,\n                padding=padding,\n            )\n        )\n\n        self.layers.append(nn.LeakyReLU(0.2))\n\n        # the rest of the layers\n        for in_out in range(1, (len(hdim) - 1)):\n            self.layers.append(\n                nn.Conv2d(\n                    in_channels=hdim[in_out],\n                    out_channels=hdim[(in_out + 1)],\n                    kernel_size=kernel_size,\n                    stride=stride,\n                    padding=padding,\n                )\n            )\n\n            self.layers.append(nn.BatchNorm2d(hdim[in_out + 1]))\n\n            self.layers.append(nn.LeakyReLU(0.2))\n\n\n    def forward(self, x):\n\n        SkipConnections = []\n\n        for layer in self.layers:\n            # print(layer)\n            x = layer.forward(x)\n            SkipConnections.append(x)\n\n        return x, SkipConnections","metadata":{"execution":{"iopub.status.busy":"2024-04-04T04:23:32.763653Z","iopub.execute_input":"2024-04-04T04:23:32.764320Z","iopub.status.idle":"2024-04-04T04:23:32.798342Z","shell.execute_reply.started":"2024-04-04T04:23:32.764276Z","shell.execute_reply":"2024-04-04T04:23:32.797423Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#decoder\nclass Decoder(nn.Module):\n    def __init__(self, latent_dims, s_img, hdim_in, hdim_out, kernel_size = (4,4), stride =2, padding= 1):\n        super(Decoder, self).__init__()\n\n        self.layers = nn.ModuleList()\n\n        # layers 1 to 3 with dropout\n        for in_out in range(0, 3):\n            self.layers.append(\n                nn.ConvTranspose2d(\n                    in_channels=hdim_in[in_out],\n                    out_channels=hdim_out[(in_out + 1)],\n                    kernel_size=kernel_size,\n                    stride=stride,\n                    padding=padding,\n                )\n            )\n\n            self.layers.append(nn.BatchNorm2d(hdim_out[in_out + 1]))\n            self.layers.append(nn.Dropout2d(0.5))\n            self.layers.append(nn.ReLU())\n\n        # the rest of the layers \n        for in_out in range(3, (len(hdim_in) - 1)):\n            self.layers.append(\n                nn.ConvTranspose2d(\n                    in_channels=hdim_in[in_out],\n                    out_channels=hdim_out[(in_out + 1)],\n                    kernel_size=kernel_size,\n                    stride=stride,\n                    padding=padding,\n                )\n            )\n\n            self.layers.append(nn.BatchNorm2d(hdim_out[in_out + 1]))\n            self.layers.append(nn.Identity())\n#             self.layers.append(nn.Dropout2d(0.5))\n            self.layers.append(nn.ReLU())\n        \n        \n            \n    def forward(self, z, SkipConnections):\n        \n        EncoderIndex = 3/4\n        SkipConnections.reverse()\n\n        for i, layer in enumerate(self.layers):\n            if i % 4 == 0 and i != 0:\n                j = int(EncoderIndex * i)\n                z = layer.forward(torch.add(z, SkipConnections[j]))\n#                 z = layer.forward(torch.cat((z,SkipConnections[j]), 1)) \n                    \n            else:\n                z = layer.forward(z)\n                \n        z = torch.tanh(z)\n#         z = z * 255\n        return z","metadata":{"execution":{"iopub.status.busy":"2024-04-04T04:23:32.799718Z","iopub.execute_input":"2024-04-04T04:23:32.800190Z","iopub.status.idle":"2024-04-04T04:23:32.814026Z","shell.execute_reply.started":"2024-04-04T04:23:32.800160Z","shell.execute_reply":"2024-04-04T04:23:32.813360Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Generator\nclass Generator(nn.Module):\n    def __init__(self,\n                latent_dims,\n                s_img,\n                hdim_e,\n                hdim_d_input,\n                hdim_d_output,\n                kernel_size, \n                padding):\n        super(Generator, self).__init__()\n\n        self.encoder = Encoder(\n            latent_dims=latent_dims,\n            s_img=s_img,hdim=hdim_e,\n            kernel_size= kernel_size,\n            padding= padding)\n        self.decoder = Decoder(\n            latent_dims=latent_dims,\n            s_img=s_img, \n            hdim_in=hdim_d_input, \n            hdim_out=hdim_d_output,\n            kernel_size=kernel_size, \n            padding=padding)\n\n    def forward(self, x):\n\n        z, skipConnections = self.encoder(x)\n        # print(f\"the shape of encoder is {z.shape}\")\n        y = self.decoder(z, skipConnections)\n\n        return y","metadata":{"execution":{"iopub.status.busy":"2024-04-04T04:23:32.815231Z","iopub.execute_input":"2024-04-04T04:23:32.815582Z","iopub.status.idle":"2024-04-04T04:23:32.829979Z","shell.execute_reply.started":"2024-04-04T04:23:32.815551Z","shell.execute_reply":"2024-04-04T04:23:32.829195Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    \n    def __init__(self, latent_dims, s_img, hdim, kernel_size=(4, 4), stride=2):\n        \n        super(Discriminator, self).__init__()\n\n        ########################################################################\n        #    Create the necessary layers                                 #\n        ########################################################################\n\n        self.layers = nn.ModuleList()\n        \n        # Input layer dim -- down1\n        self.layers.append(nn.Conv2d(in_channels=6, out_channels=64, kernel_size=kernel_size, stride=2, padding=1))\n\n        # Hidden to hidden convolution -- down2 and down 3\n        for i in range(0, 2):\n            self.layers.append(nn.Conv2d(in_channels=hdim[i],\n                                             out_channels=hdim[i + 1],\n                                             kernel_size=kernel_size, stride = stride, padding=1))\n\n        # Pad with zeroes\n        self.layers.append(nn.ZeroPad2d(padding=(1,1,1,1)))\n\n        # Conv2D\n        self.layers.append(nn.Conv2d(in_channels=hdim[3],\n                                             out_channels=hdim[4],\n                                             kernel_size=kernel_size, stride = 1))\n\n        # Batchnorm\n        self.layers.append(nn.BatchNorm2d(hdim[4]))\n\n        # Zeropad2\n        self.layers.append(nn.ZeroPad2d(padding=(1,1,1,1)))\n\n        #Conv2D 2\n        self.layers.append(nn.Conv2d(in_channels=hdim[5],\n                                             out_channels=hdim[6],\n                                             kernel_size=kernel_size, stride = 1))\n\n        self.Leakyrelu = nn.LeakyReLU(0.2)\n        \n    def forward(self, x):\n\n        for n_layer, layer in enumerate(self.layers):\n            ## The fourth layer first has a batchnorm and then a Leakyrelu\n            if n_layer != 4:\n                x = self.Leakyrelu(layer(x))\n            else:\n                x = layer(x)\n        return x\n        ","metadata":{"execution":{"iopub.status.busy":"2024-04-04T04:23:32.831209Z","iopub.execute_input":"2024-04-04T04:23:32.832055Z","iopub.status.idle":"2024-04-04T04:23:32.845263Z","shell.execute_reply.started":"2024-04-04T04:23:32.832030Z","shell.execute_reply":"2024-04-04T04:23:32.844423Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def train_gan(train_loader, net_gen, net_dis, optimizer_gen, optimizer_dis, criterion_l1, criterion_bce, pin_memory , device = 'cpu'):\n    \"\"\"\n    Trains network for one epoch in batches.\n\n    Args:\n        train_loader: Data loader for training set.\n        net: Neural network model.\n        optimizer: Optimizer (e.g. SGD).\n        criterion: Loss function (e.g. cross-entropy loss).\n        device: whether the network runs on cpu or gpu\n    \"\"\"\n    \n#     labels = torch.zeros(30, 30)\n\n    avg_loss_gen = 0\n    avg_loss_dis = 0\n\n\n    # iterate through batches\n    for i, data in enumerate(train_loader):\n        \n        use_generator = False\n        \n        # get the inputs; data is a list of [inputs, labels]\n        under_water_img, ground_truth = data\n        \n        # convert the inputs to run on GPU if set\n        if device != 'cpu':\n            under_water_img, ground_truth = under_water_img.cuda(non_blocking=pin_memory), ground_truth.cuda(non_blocking=pin_memory)\n            \n\n        # zero the parameter gradients\n        optimizer_gen.zero_grad()\n        optimizer_dis.zero_grad()\n        \n        loss_gen_tot = 0\n        outputs_gen = net_gen.forward(under_water_img)\n    \n#########loss generator############################################################################################################################\n\n        # forward + backward + optimize\n        \n        inputs_dis_fake = torch.cat((under_water_img, outputs_gen), dim = 1)\n        outputs_dis_fake = net_dis.forward(inputs_dis_fake)\n        \n        loss_gen_bce = criterion_bce(torch.sigmoid(outputs_dis_fake), torch.ones_like(outputs_dis_fake))# Dit is niet kapot!!!!!!!\n        loss_gen_l1 = torch.mul(criterion_l1(outputs_gen,ground_truth),100)\n        loss_gen_tot = torch.add(loss_gen_bce, loss_gen_l1)\n        loss_gen_tot.backward()\n        optimizer_gen.step()\n        \n####################################################################################################################################################\n\n\n\n#########loss discriminator#########################################################################################################################\n            \n        loss_dis = 0\n    \n        n_samples, channels, s_img, _ = outputs_gen.size()\n    \n        ones = torch.ones(n_samples, 1, 30, 30)\n        zeros = torch.zeros(n_samples, 1, 30, 30)\n        labels = torch.cat((zeros, ones), dim = 0).to(device)\n\n        set_fake = inputs_dis_fake.detach()\n        set_true = torch.cat((under_water_img, ground_truth), dim = 1)\n        x = torch.cat((set_fake, set_true), dim = 0).to(device)\n\n        outputs = net_dis.forward(x)\n        output_probabilities = torch.sigmoid(outputs)\n\n        loss_dis_tot = criterion_bce(output_probabilities, labels)\n        loss_dis_tot.backward()\n        optimizer_dis.step()\n        \n#########loss discriminator#########################################################################################################################\n\n        # keep track of loss and accuracy\n        avg_loss_gen += loss_gen_tot\n        avg_loss_dis += loss_dis_tot\n        \n    return avg_loss_gen/len(train_loader), avg_loss_dis/len(train_loader), \n#     return avg_loss_gen/len(train_loader), avg_loss_dis/len(train_loader), \n\ndef try_gpu():\n    \"\"\"\n    If GPU is available, return torch.device as cuda:0; else return torch.device\n    as cpu.\n    \"\"\"\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    else:\n        device = torch.device('cpu')\n    return device","metadata":{"execution":{"iopub.status.busy":"2024-04-04T04:23:32.846422Z","iopub.execute_input":"2024-04-04T04:23:32.846741Z","iopub.status.idle":"2024-04-04T04:23:32.862628Z","shell.execute_reply.started":"2024-04-04T04:23:32.846697Z","shell.execute_reply":"2024-04-04T04:23:32.861701Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # show_images()\n    torch.cuda.empty_cache()\n    num_workers = 4\n    pin_memory = True\n    batch_size = 64\n    n_samples, in_channels, s_img, latent_dims, padding = 1, 3, 256, 512,1\n    hdim_e = [3, 64, 128, 256, 512, 512, 512, 512, 512] #choose hidden dimension encoder\n    hdim_d_output = [512, 512, 512, 512, 512, 256, 128, 64, 3]\n#     hdim_d_input = [512, 1024, 1024, 1024, 1024, 512, 256, 128, 3] #choose hidden dimension decoder\n    hdim_d_input = hdim_d_output\n    in_channels_dis = 6 # 6 for two images\n    hdim_dis = [64, 128, 256, 256, 512, 512, 1] #choose hidden dimension discriminator\n    output_shape = (n_samples, 1, 30, 30)\n    \n    kernel_size = (4,4)\n    \n    # Define your transformations\n    transform = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n   # Define the transformation pipeline with added jitter\n    transform = transforms.Compose([\n        transforms.Resize(256, interpolation=transforms.InterpolationMode.NEAREST),  # Resize to 256x256 using nearest neighbor method\n        transforms.RandomCrop(256),  # Apply random cropping to introduce jitter\n        transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally\n        # Random jitter through changes in brightness, contrast, saturation, and hue\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02),\n        # Random affine transformations for scaling and rotation\n        transforms.RandomAffine(degrees=10, scale=(0.95, 1.05)),\n        transforms.ToTensor(),  # Convert images to tensor\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n    ])\n\n    # Initialize the dataset\n    paired_dataset_dark = PairedImageDataset(rootA=r'/kaggle/input/EUVP/Paired/underwater_dark/trainA',\n                                        rootB=r'/kaggle/input/EUVP/Paired/underwater_dark/trainB',\n                                        transform=transform)\n    # Initialize the dataset\n    paired_dataset_imagenet = PairedImageDataset(rootA=r'/kaggle/input/EUVP/Paired/underwater_imagenet/trainA',\n                                        rootB=r'/kaggle/input/EUVP/Paired/underwater_imagenet/trainB',\n                                        transform=transform)\n\n    # Initialize the dataset\n    paired_dataset_scenes = PairedImageDataset(rootA=r'/kaggle/input/EUVP/Paired/underwater_scenes/trainA',\n                                        rootB=r'/kaggle/input/EUVP/Paired/underwater_scenes/trainB',\n                                        transform=transform)\n\n    # Initialize loader\n    EUVP_data = DataLoader(\n        paired_dataset_dark,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=pin_memory)\n    \n        # Initialize DataLoader\n    EUVP_data1 = DataLoader(\n        paired_dataset_imagenet,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=pin_memory)\n    \n        # Initialize DataLoader\n    EUVP_data2 = DataLoader(\n        paired_dataset_scenes,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=pin_memory)\n\n   # Fetch a single batch\n    images_a, images_b = next(iter(EUVP_data))\n#     _ , img_x = EUVP_data\n\n    #Set the number of dimensions of the latent space\n    # latent_dims = [2,3]\n    s_img = np.size(images_a[1][0], axis = 1) #get image size (height = width) from a data sample\n\n    #initialize generator model\n    model_gen = Generator(latent_dims=latent_dims,\n                        s_img=s_img,\n                        hdim_e=hdim_e, \n                        hdim_d_input=hdim_d_input,\n                        hdim_d_output=hdim_d_output, \n                        padding=padding,\n                        kernel_size=kernel_size)\n    #intit dis\n    model_dis = Discriminator(latent_dims, s_img, hdim_dis)\n\n    # Create a writer to write to Tensorboard\n    writer = SummaryWriter()\n\n    #Create instance of Autoencoder\n    device = try_gpu()\n    print(device)\n    \n    if torch.cuda.is_available():\n        model_gen= nn.DataParallel(model_gen)\n        model_gen.to(device)\n        model_dis = nn.DataParallel(model_dis)\n        model_dis.to(device)\n\n    # Create loss function and optimizer\n    criterion_l1 = F.l1_loss \n    criterion_bce = F.binary_cross_entropy\n    \n    optimizer_gen = optim.Adam(model_gen.parameters(), lr=2e-4, betas=(0.5, 0.999))\n    optimizer_dis = optim.Adam(model_dis.parameters(), lr=2e-4, betas=(0.5, 0.999))\n\n    # Set the number of epochs to for training\n    epochs = 150\n    \n    plt.ion()  # Enable interactive mode\n    \n    losses_gen = []\n    losses_dis = [] \n    \n    for epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n        # Train on data\n        loss1 = 0\n        loss2 = 0\n        count = 1\n        \n        train_loss_gen, train_loss_dis = train_gan(EUVP_data, model_gen,model_dis, optimizer_gen, optimizer_dis, criterion_l1, criterion_bce, pin_memory, device)\n        \n        #########All data########################################################################################################################################\n       \n        loss1 += train_loss_gen.item()\n        loss2 += train_loss_dis.item()\n        count += 1\n\n        train_loss_gen, train_loss_dis = train_gan(EUVP_data1, model_gen,model_dis, optimizer_gen, optimizer_dis, criterion_l1, criterion_bce, pin_memory, device)\n\n        loss1 += train_loss_gen.item()\n        loss2 += train_loss_dis.item()\n        count += 1\n\n        train_loss_gen, train_loss_dis = train_gan(EUVP_data2, model_gen,model_dis, optimizer_gen, optimizer_dis, criterion_l1, criterion_bce, pin_memory, device)\n\n        ##############################################################################################################################################################\n        \n        # Write metrics to Tensorboard\n        writer.add_scalars(\"Loss\", {'Train': train_loss_gen}, epoch)\n        writer.add_scalars(\"Loss\", {'Disc': train_loss_dis}, epoch)\n\n        losses_gen.append((train_loss_gen.item() + loss1) / count)\n        losses_dis.append((train_loss_dis.item() + loss2) / count)\n        \n#         Visualization code\n        clear_output(wait=True)\n        plt.figure(figsize=(10, 5))\n        plt.plot(losses_gen, label='Training Loss Generator')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.title('Training Loss Over Time')\n        plt.legend()\n        plt.show()\n        \n        plt.figure(figsize=(10, 5))\n        plt.plot(losses_dis, label='Training Loss Discriminator', color='orange')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.title('Training Loss Over Time')\n        plt.legend()\n        plt.show()\n        \n    # save\n    torch.save(model_gen, 'model.pth')\n    torch.save(model_gen.state_dict(), 'model_dict.pth')\n        # save\n    torch.save(model_dis, 'model_dis.pth')\n    torch.save(model_dis.state_dict(), 'model_dict_dis.pth')\n    \n    plt.ioff()  # Disable interactive mode\n    writer.close()","metadata":{"execution":{"iopub.status.busy":"2024-04-04T04:23:32.863879Z","iopub.execute_input":"2024-04-04T04:23:32.864404Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/150 [00:00<?, ?it/s]","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.empty_cache()\n# make a folder\ncount = 0\nimage_counter = 10\npath = '/kaggle/working/test2'\npath_loop = None\n# Initialize DataLoader\nEUVP_data = DataLoader(paired_dataset_dark, batch_size=200, shuffle=False, num_workers=4)\n\nprint(\"start printing\")\n# Store somep images\nfor i, data in enumerate(EUVP_data):\n    inputs, labels = data\n    inputs, labels = inputs.to(device), labels.to(device)\n    y_out = model_gen.forward(inputs)\n#     y_out = torch.tanh(y_out)\n\n    for x in range(len(inputs) - 1):\n        path_loop = f\"{path}/test{count}\"\n        if not (os.path.exists(path_loop) and os.path.isdir(path_loop)):\n            os.makedirs(path_loop)\n\n        name = (f'{path_loop}/image_in{count}.jpg')\n        save_image(inputs[x], name)\n        name = (f'{path_loop}/image_out{count}.jpg')\n        save_image(y_out[x], name)\n        name = (f'{path_loop}/image_truth{count}.jpg')\n        save_image(labels[x], name)\n        count += 1\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import zipfile\nimport os\nfrom IPython.display import FileLink\n\ntorch.cuda.empty_cache()\ndef zip_dir(directory = os.curdir, file_name = 'resutls.zip'):\n    \"\"\"\n    zip all the files in a directory\n    \n    Parameters\n    _____\n    directory: str\n        directory needs to be zipped, defualt is current working directory\n        \n    file_name: str\n        the name of the zipped file (including .zip), default is 'directory.zip'\n        \n    Returns\n    _____\n    Creates a hyperlink, which can be used to download the zip file)\n    \"\"\"\n    os.chdir(directory)\n    zip_ref = zipfile.ZipFile(file_name, mode='w')\n    for folder, _, files in os.walk(directory):\n        for file in files:\n            if file_name in file:\n                pass\n            else:\n                zip_ref.write(os.path.join(folder, file))\n\n    return FileLink(file_name)\n\nzip_dir()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir /kaggle/working/runs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" %reload_ext tensorboard","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!kill 292","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}