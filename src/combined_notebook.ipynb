{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c15bfa5f-3734-43be-95e7-bd8a8ed5efaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 6\u001b[1;36m\n\u001b[1;33m    import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\u001b[1;36m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m\u001b[1;31m:\u001b[0m No module named 'pandas'\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import zipfile\n",
    "import os\n",
    "from IPython.display import FileLink\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import  DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#          print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f52b0-d083-42c9-8c56-bf7290abb56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dir_in_matthijs = r'C:\\Users\\matth\\Documents\\Master Nanobiology\\Deep learning\\project\\Images input'\n",
    "folder_dir_gt_matthijs = r\"C:\\Users\\matth\\Documents\\Master Nanobiology\\Deep learning\\project\\Images GT\"\n",
    "folder_dir_en_matthijs = r\"C:\\Users\\matth\\Documents\\Master Nanobiology\\Deep learning\\project\\Images G\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0bfa50-f603-4412-b14c-73a8b6e04e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dims, s_img, hdim, kernel_size=(4, 4), stride=2):\n",
    "        \n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        ########################################################################\n",
    "        #    Create the necessary layers                                 #\n",
    "        ########################################################################\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Input layer dim -- down1\n",
    "        self.layers.append(nn.Conv2d(in_channels=6, out_channels=64, kernel_size=kernel_size, stride=2, padding=1))\n",
    "\n",
    "        # Hidden to hidden convolution -- down2 and down 3\n",
    "        for i in range(0, 2):\n",
    "            self.layers.append(nn.Conv2d(in_channels=hdim[i],\n",
    "                                             out_channels=hdim[i + 1],\n",
    "                                             kernel_size=kernel_size, stride = stride, padding=1))\n",
    "\n",
    "        # Pad with zeroes\n",
    "        self.layers.append(nn.ZeroPad2d(padding=(1,1,1,1)))\n",
    "\n",
    "        # Conv2D\n",
    "        self.layers.append(nn.Conv2d(in_channels=hdim[3],\n",
    "                                             out_channels=hdim[4],\n",
    "                                             kernel_size=kernel_size, stride = 1))\n",
    "\n",
    "        # Batchnorm\n",
    "        self.layers.append(nn.BatchNorm2d(hdim[4]))\n",
    "\n",
    "        # Zeropad2\n",
    "        self.layers.append(nn.ZeroPad2d(padding=(1,1,1,1)))\n",
    "\n",
    "        #Conv2D 2\n",
    "        self.layers.append(nn.Conv2d(in_channels=hdim[5],\n",
    "                                             out_channels=hdim[6],\n",
    "                                             kernel_size=kernel_size, stride = 1))\n",
    "\n",
    "        self.Leakyrelu = nn.LeakyReLU(0.2)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for n_layer, layer in enumerate(self.layers):\n",
    "            ## The fourth layer first has a batchnorm and then a Leakyrelu\n",
    "            if n_layer != 4:\n",
    "                x = self.Leakyrelu(layer(x))\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65d19e5-33f3-48bd-98c8-410dea55cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator():\n",
    "\n",
    "    folder_dir_gt = folder_dir_gt_matthijs\n",
    "    folder_dir_in = folder_dir_in_matthijs\n",
    "    folder_dir_en = folder_dir_en_matthijs\n",
    "\n",
    "    in_list = [os.path.join(folder_dir_in, filename) for filename in os.listdir(folder_dir_en)]\n",
    "    gt_list = [os.path.join(folder_dir_gt, filename) for filename in os.listdir(folder_dir_gt)]\n",
    "    enhanced_list = [os.path.join(folder_dir_en, filename) for filename in os.listdir(folder_dir_in)]\n",
    "\n",
    "    # =============================================================================\n",
    "    # test auo-encoder\n",
    "    # =============================================================================\n",
    "    n_samples, in_channels, s_img, latent_dims = len(in_list) * 2, 6, 256, 512 # 6 for two images\n",
    "    hdim = [64, 128, 256, 256, 512, 512, 1] #choose hidden dimension discriminator\n",
    "    output_shape = (n_samples, 1, 30, 30)\n",
    "\n",
    "    # =============================================================================\n",
    "    # Fill x with stacks of fake images (random) and real images (all ones)\n",
    "    # =============================================================================\n",
    "    # real_image = torch.ones(in_channels//2, s_img, s_img)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256), transforms.InterpolationMode.NEAREST),  # Resize the images to a consistent size\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize the images\n",
    "        transforms.ToTensor()           # Convert the images to PyTorch tensors\n",
    "    ])\n",
    "\n",
    "\n",
    "    training_list = []\n",
    "\n",
    "    # create training list with right size (transformed)\n",
    "    for i in range(len(in_list)):\n",
    "        input_img = transform(Image.open(in_list[i]))\n",
    "        enhanced_img = transform(Image.open(enhanced_list[i]))\n",
    "        gt_img = transform(Image.open(gt_list[i]))\n",
    "\n",
    "        # Normalize range [-1, 1]\n",
    "        input_img = torch.tanh(input_img)\n",
    "        enhanced_img = torch.tanh(enhanced_img)\n",
    "        gt_img = torch.tanh(gt_img)\n",
    "\n",
    "        # List image combinations with label\n",
    "        training_list.append((input_img, enhanced_img, False))\n",
    "        training_list.append((input_img, gt_img, True))\n",
    "\n",
    "    random.shuffle(training_list)\n",
    "\n",
    "    x = torch.empty((n_samples, in_channels, s_img, s_img))\n",
    "    labels = torch.empty(output_shape)\n",
    "\n",
    "\n",
    "    for i, (img1, img2, is_real) in enumerate(training_list):\n",
    "\n",
    "        x[i, :, :, :] = torch.cat((img1, img2), dim=0)\n",
    "\n",
    "        if is_real:\n",
    "            labels[i, 0] = torch.ones(30, 30)\n",
    "        else:\n",
    "            labels[i, 0] = torch.zeros(30, 30)\n",
    "\n",
    "\n",
    "    # =============================================================================\n",
    "    # initialize model\n",
    "    # =============================================================================\n",
    "    model = Discriminator(latent_dims, s_img, hdim)\n",
    "    summary(model, (in_channels, s_img, s_img), device='cpu') # (in_channels, height, width)\n",
    "\n",
    "    # =============================================================================\n",
    "    # training of the Discriminator\n",
    "    # =============================================================================\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "    n_epochs = 2\n",
    "    losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x)\n",
    "        output_probabilities = torch.sigmoid(outputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output_probabilities, labels)\n",
    "        print(f'Epoch [{epoch + 1}/{n_epochs}], Loss: {loss.item()}')\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # =============================================================================\n",
    "    # plot the training Loss over epochs (on its own data,\n",
    "    # so will eventually convege to 0)\n",
    "    # =============================================================================\n",
    "\n",
    "    plt.plot(losses)\n",
    "    plt.ylim(0, .8)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('Loss (BCE)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace44cf-39f5-470d-8fb9-22a5e26164b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(input_imgs, real_imgs):\n",
    "\n",
    "    n_samples, in_channels, s_img, latent_dims = len(input_imgs), 6, 256, 512 # 6 for two images\n",
    "    hdim = [64, 128, 256, 256, 512, 512, 1] #choose hidden dimension discriminator\n",
    "    x_generator = torch.empty((n_samples, in_channels, s_img, s_img))\n",
    "    x_discriminator = torch.empty((n_samples, in_channels, s_img, s_img))\n",
    "\n",
    "    generator = Generator() # TODO:\n",
    "    discriminator  = Discriminator(latent_dims, s_img, hdim)\n",
    "    loss_L1 = nn.L1Loss()\n",
    "    loss_BCE = nn.BSELoss()\n",
    "    optimizer = optim.Adam(generator.parameters(), lr=2e-4, betas = (0.5, 0.999))\n",
    "\n",
    "    for i in range(len(input_imgs)):\n",
    "        real_img, input_img = real_imgs[i], input_imgs[i]\n",
    "        x_generator[i, :, :, :] = input_img\n",
    "        \n",
    "    generator_out = generator.forward(x_generator)\n",
    "\n",
    "    x_discriminator = torch.cat((x_generator, generator_out), dim=0)\n",
    "\n",
    "    # Right Column\n",
    "    discriminator_out = discriminator.forward(x_discriminator)\n",
    "\n",
    "    loss_1 = loss_L1(generator_out, real_img)\n",
    "    loss_2 = loss_BCE(discriminator_out, torch.ones(30, 30))\n",
    "\n",
    "    total_loss = 100*loss_1 + loss_2\n",
    "\n",
    "    #Apply Gradients\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return total_loss.item()\n",
    "\n",
    "\n",
    "folder_dir_gt = folder_dir_in_matthijs\n",
    "folder_dir_in = folder_dir_gt_matthijs\n",
    "\n",
    "in_list = [os.path.join(folder_dir_in, filename) for filename in os.listdir(folder_dir_in)]\n",
    "gt_list = [os.path.join(folder_dir_gt, filename) for filename in os.listdir(folder_dir_gt)]\n",
    "enhanced_list = [os.path.join(folder_dir_in, filename) for filename in os.listdir(folder_dir_in)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeafc5c-2c8b-4b11-9e04-ce7342c3d640",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb959ae4-5cdf-4873-89f5-e30a28824f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedImageDataset(Dataset):\n",
    "    def __init__(self, rootA, rootB, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rootA (string): Directory with all the images in trainA.\n",
    "            rootB (string): Directory with all the images in trainB.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.rootA = rootA\n",
    "        self.rootB = rootB\n",
    "        self.transform = transform\n",
    "\n",
    "        # Assuming filenames in both folders are the same and in order\n",
    "        self.filenames = sorted(os.listdir(rootA))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_nameA = os.path.join(self.rootA, self.filenames[idx])\n",
    "        imageA = Image.open(img_nameA).convert('RGB')\n",
    "\n",
    "        img_nameB = os.path.join(self.rootB, self.filenames[idx])\n",
    "        imageB = Image.open(img_nameB).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            imageA = self.transform(imageA)\n",
    "            imageB = self.transform(imageB)\n",
    "\n",
    "        return imageA, imageB\n",
    "\n",
    "def try_gpu():\n",
    "    \"\"\"\n",
    "    If GPU is available, return torch.device as cuda:0; else return torch.device\n",
    "    as cpu.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # test\n",
    "        dim_in_encoder = [3, 64, 128, 256, 512, 512, 512, 512, 512, 512]\n",
    "        dim_in_decoder = [512, 512, 512, 512, 512, 256, 128, 64, 3]\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        ####################################################################################\n",
    "        # encoder\n",
    "        ####################################################################################\n",
    "\n",
    "        # layer 1\n",
    "        self.layers.append(\n",
    "            nn.Conv2d(\n",
    "                in_channels=dim_in_encoder[0],\n",
    "                out_channels=dim_in_encoder[1],\n",
    "                kernel_size=(4, 4),\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.layers.append(nn.LeakyReLU(0.2))\n",
    "\n",
    "        # the rest of the layers\n",
    "        for in_out in range(1, (len(dim_in_encoder) - 1)):\n",
    "            self.layers.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=dim_in_encoder[in_out],\n",
    "                    out_channels=dim_in_encoder[(in_out + 1)],\n",
    "                    kernel_size=(4, 4),\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.layers.append(nn.BatchNorm2d(dim_in_encoder[in_out + 1]))\n",
    "\n",
    "            self.layers.append(nn.LeakyReLU(0.2))\n",
    "\n",
    "        print(\"layer\", len(self.layers))\n",
    "\n",
    "        ####################################################################################\n",
    "        # decoder\n",
    "        ####################################################################################\n",
    "\n",
    "        # layers 1 to 3 with dropout\n",
    "        for in_out in range(0, 3):\n",
    "            self.layers.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels=dim_in_decoder[in_out],\n",
    "                    out_channels=dim_in_decoder[(in_out + 1)],\n",
    "                    kernel_size=(4, 4),\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.layers.append(nn.BatchNorm2d(dim_in_decoder[in_out + 1]))\n",
    "            self.layers.append(nn.Dropout2d(0.5))\n",
    "            self.layers.append(nn.LeakyReLU(0.2))\n",
    "\n",
    "        # the rest of the layers without dropout\n",
    "        for in_out in range(3, (len(dim_in_decoder) - 1)):\n",
    "            self.layers.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels=dim_in_decoder[in_out],\n",
    "                    out_channels=dim_in_decoder[(in_out + 1)],\n",
    "                    kernel_size=(4, 4),\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.layers.append(nn.BatchNorm2d(dim_in_decoder[in_out + 1]))\n",
    "            self.layers.append(nn.Dropout2d(0)) #todo()\n",
    "            self.layers.append(nn.LeakyReLU(0.2))\n",
    "\n",
    "        print(\"layer\", len(self.layers))\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder_out = []\n",
    "\n",
    "        #forward on first layer\n",
    "        x = self.forward_2_lay(x, 0)\n",
    "        encoder_out.append(x)\n",
    "        # print(\"first pass\", x.shape)\n",
    "\n",
    "        #forward encoder and save layers\n",
    "        for i in range(0, 6): # 1 - 8\n",
    "            x = self.forward_3_lay(x, i, 2)\n",
    "            encoder_out.append(x)\n",
    "            # print(\"second loop\", x.shape)\n",
    "\n",
    "        #Do layer 512x1x1\n",
    "        x = self.forward_3_lay(x, 6, 2)\n",
    "        #Do layer 512x2x2\n",
    "        x = self.forward_4_lay(x, 0, 26)\n",
    "\n",
    "        #Do forward on decoder\n",
    "        for i in range(0, 7):\n",
    "            # print(\"encoder\", encoder_out[6 - i].shape,\"x\", x.shape)\n",
    "            x = self.forward_4_lay(torch.add(x, encoder_out[6 - i]), i, 30)\n",
    "            # print(\"decoder for loop\", x.shape)\n",
    "\n",
    "        x = torch.tanh(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "    def forward_2_lay(self, x, layer_num):\n",
    "\n",
    "        for i in range(0, 2):\n",
    "            x = self.layers[i + 2 * layer_num](x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_3_lay(self, x, layer_count, start):\n",
    "\n",
    "        for i in range(0, 3):\n",
    "            x = self.layers[i + 3 * layer_count + start](x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_4_lay(self, x, layer_num, start):\n",
    "        # print(len(self.layers))\n",
    "        for i in range(0, 4):\n",
    "            # print(i + 4 * layer_num)\n",
    "            x = self.layers[i + 4 * layer_num + start](x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def train(train_loader, net, optimizer, criterion, device='cpu'):\n",
    "    \"\"\"\n",
    "    Trains network for one epoch in batches.\n",
    "\n",
    "    Args:\n",
    "        train_loader: Data loader for training set.\n",
    "        net: Neural network model.\n",
    "        optimizer: Optimizer (e.g. SGD).\n",
    "        criterion: Loss function (e.g. cross-entropy loss).\n",
    "        device: whether the network runs on cpu or gpu\n",
    "    \"\"\"\n",
    "\n",
    "    avg_loss = 0\n",
    "\n",
    "    # iterate through batches\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, good_images = data\n",
    "\n",
    "        # convert the inputs to run on GPU if set\n",
    "        inputs, good_images = inputs.to(device), good_images.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        loss = criterion(outputs, good_images) # compare output with labels\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # keep track of loss and accuracy\n",
    "        avg_loss += loss\n",
    "\n",
    "    return avg_loss / len(train_loader)\n",
    "\n",
    "def run_saved():\n",
    "    # Define your transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # Initialize the dataset\n",
    "    paired_dataset_dark = PairedImageDataset(rootA=r'/kaggle/input/euvp-dataset/EUVP/Paired/underwater_dark/trainA',\n",
    "                                        rootB=r'/kaggle/input/euvp-dataset/EUVP/Paired/underwater_dark/trainB',\n",
    "                                        transform=transform)\n",
    "\n",
    "    # Create instance of Autoencoder\n",
    "    device = try_gpu()\n",
    "\n",
    "    model = torch.load('model.pth')\n",
    "#     model_saved.load('saved_model_full.pth')\n",
    "     # Load state dictionary  # make a folder\n",
    "    \n",
    "    count = 0\n",
    "    image_counter = 10\n",
    "    path = 'saved'\n",
    "    path_loop = None\n",
    "    # Initialize DataLoader\n",
    "    EUVP_data = DataLoader(paired_dataset_dark, batch_size=5, shuffle=False, num_workers=16)\n",
    "    print(\"start printing\")\n",
    "    \n",
    "    path_ImageA = f\"ImageA\"\n",
    "    path_ImageB = f\"ImageB\"\n",
    "    path_ImageG = f\"ImageG\"\n",
    "    \n",
    "    if not (os.path.exists(path_loop) and os.path.isdir(path_loop)):\n",
    "        os.makedirs(path_loop)\n",
    "    \n",
    "    # Store somep images\n",
    "    for i, data in enumerate(EUVP_data):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        y_out = model_saved.forward(inputs)\n",
    "\n",
    "        for x in range(len(inputs)):\n",
    "            name = (f'{path_ImageA}/image{count}.jpg')\n",
    "            save_image(inputs[x], name)\n",
    "            name = (f'{path_ImageB}/image{count}.jpg')\n",
    "            save_image(y_out[x], name)\n",
    "            name = (f'{path_ImageG}/image{count}.jpg')\n",
    "            save_image(labels[x], name)\n",
    "            count += 1\n",
    "\n",
    "            if (count == image_counter):\n",
    "                break\n",
    "        break\n",
    "\n",
    "def main():\n",
    "    torch.cuda.empty_cache()\n",
    "    # Define your transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # Initialize the dataset\n",
    "    paired_dataset_dark = PairedImageDataset(rootA=r'/kaggle/input/euvp-dataset/EUVP/Paired/underwater_dark/trainA',\n",
    "                                        rootB=r'/kaggle/input/euvp-dataset/EUVP/Paired/underwater_dark/trainB',\n",
    "                                        transform=transform)\n",
    "    # Initialize the dataset\n",
    "    paired_dataset_imagenet = PairedImageDataset(rootA=r'/kaggle/input/euvp-dataset/EUVP/Paired/underwater_imagenet/trainA',\n",
    "                                        rootB=r'/kaggle/input/euvp-dataset/EUVP/Paired/underwater_imagenet/trainB',\n",
    "                                        transform=transform)\n",
    "\n",
    "    # Initialize the dataset\n",
    "    paired_dataset_scenes = PairedImageDataset(rootA=r'/kaggle/input/euvp-dataset/EUVP/Paired/underwater_scenes/trainA',\n",
    "                                        rootB=r'/kaggle/input/euvp-dataset/EUVP/Paired/underwater_scenes/trainB',\n",
    "                                        transform=transform)\n",
    "    \n",
    "    # Initialize DataLoader\n",
    "    EUVP_data = DataLoader(paired_dataset_dark, batch_size=360, shuffle=True, num_workers=8)\n",
    "    \n",
    "        # Initialize DataLoader\n",
    "    EUVP_data1 = DataLoader(paired_dataset_imagenet, batch_size=360, shuffle=True, num_workers=8)\n",
    "    \n",
    "        # Initialize DataLoader\n",
    "    EUVP_data2 = DataLoader(paired_dataset_scenes, batch_size=360, shuffle=True, num_workers=8)\n",
    "\n",
    "    # initialize model\n",
    "    model = Generator()\n",
    "\n",
    "    # Create a writer to write to Tensorboard\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # Create instance of Autoencoder\n",
    "    device = try_gpu()\n",
    "    print(device)\n",
    "    if torch.cuda.is_available():\n",
    "        model = nn.DataParallel(model)\n",
    "        model.cuda()\n",
    "    # AE2 = Generator(latent_dims[0], s_img, hdim = hdim).to(device) #2-dimensional latent space\n",
    "    # AE3 = Generator(latent_dims[1], s_img, hdim = hdim).to(device) #3-dimensional latent space\n",
    "\n",
    "    # Create loss function and optimizer\n",
    "    criterion = F.mse_loss\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "    # Set the number of epochs to for training\n",
    "    epochs = 20\n",
    "\n",
    "   \n",
    "    for epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n",
    "        # Train on data\n",
    "        train_loss = train_generator()\n",
    "        # Write metrics to Tensorboard\n",
    "        writer.add_scalars(\"Loss\", {'Train': train_loss}, epoch)\n",
    "        \n",
    "        # Train on data\n",
    "        train_loss = train(EUVP_data1, model, optimizer, criterion, device)\n",
    "        # Write metrics to Tensorboard\n",
    "        writer.add_scalars(\"Loss\", {'Train': train_loss}, epoch)\n",
    "        \n",
    "        # Train on data\n",
    "        train_loss = train(EUVP_data2, model, optimizer, criterion, device)\n",
    "        # Write metrics to Tensorboard\n",
    "        writer.add_scalars(\"Loss\", {'Train': train_loss}, epoch)\n",
    "    \n",
    "    # save\n",
    "    torch.save(model, 'model.pth')\n",
    "    torch.save(model.state_dict(), 'model_dict.pth')\n",
    "    \n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
